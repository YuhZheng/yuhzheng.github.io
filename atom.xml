<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YuhZheng&#39;s Geek World</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yuhzheng.github.io/"/>
  <updated>2017-09-01T01:44:14.000Z</updated>
  <id>https://yuhzheng.github.io/</id>
  
  <author>
    <name>Yuhao Zheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes on Deep Reinforcement Learning</title>
    <link href="https://yuhzheng.github.io/2017/08/31/Notes-on-Deep-Reinforcement-Learning/"/>
    <id>https://yuhzheng.github.io/2017/08/31/Notes-on-Deep-Reinforcement-Learning/</id>
    <published>2017-09-01T00:36:24.000Z</published>
    <updated>2017-09-01T01:44:14.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p><em>Last modified</em>    31 Aug, 2017 @YuhZheng</p>
<p><em>Reference</em>    “Tutorial: Deep Reinforcement Learning” by David Silver, Google Deepmind</p>
</blockquote>
<h2 id="Background-Settings"><a href="#Background-Settings" class="headerlink" title="Background Settings"></a>Background Settings</h2><p>Here is some basic notations for a typical reinforcement learning scenario: </p>
<ul>
<li>An agent takes <strong>actions</strong> ${a_t}$ in an environment. </li>
<li>The environment gives the agent <strong>rewards</strong> ${r_t}$. </li>
<li>The input of the agent from the environment is called <strong>observations</strong> ${o_t}$ </li>
</ul>
<p>We hope the agent can learn from its past experience $o_1,r_1,a_1,\dots,a_{t-1},o_t,r_t$, to improve its current <strong>state</strong> $s_t=f(o_1,r_1,a_1,\dots ,a_{t-1},o_t,r_t)$. If the environment can be fully observed, $s_t=f(o_t)$. </p>
<p>Therefore, we need to revise the policy the agent takes. It can be either </p>
<ul>
<li><p><strong>deterministic</strong>     $a=\pi(s)​$, or </p>
</li>
<li><p><strong>stochastic</strong>     $\pi(a|s)=\mathbb{P}[a|s]$.<br>​</p>
</li>
</ul>
<h2 id="Value-based-RL"><a href="#Value-based-RL" class="headerlink" title="Value-based RL"></a>Value-based RL</h2><p>We predict the future reward with a Q-value function<br>$$Q^{\pi}(s,a)=\mathbb{E}[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\dots|s,a]$$<br>which can be decomposed into a Bellman equation<br>$$Q^{\pi}(s,a)=\mathbb{E}_{s’,a’}[r+\gamma Q^{\pi}(s’,a’)|s,a]$$<br>The optimal policy can be given by $\pi^*(s)=\underset{a}{\operatorname{argmax}} Q^{*}(s,a)$, where<br>$$Q^*(s,a)=\underset{\pi}{\max}Q^{\pi}(s,a)=Q^{\pi^*}(s,a)=\mathbb{E}_{s’}[r+\gamma \underset{a’}{\max}Q^{*}(s’,a’)|s,a]$$<br>Note: the main difference between optimal Q-value and Q-value is that the Q value takes expectation w.r.t. both a’ and s’, while optimal Q-value takes expectation only w.r.t. s’, with a’ being the best action that actor can take.</p>
<h3 id="Deep-Q-Networks-DQN"><a href="#Deep-Q-Networks-DQN" class="headerlink" title="Deep Q-Networks (DQN)"></a>Deep Q-Networks (DQN)</h3><p>If we represent the value function by neural network with weights <strong>w</strong>,<br>$$Q(s,a,\textbf{w})\approx Q^*(s,a)$$<br>The objective function would be MSE loss<br>$$I=(r+\gamma \underset{a’}{\max}Q(s’,a’,\textbf{w})-Q(s,a,\textbf{w}))^2$$<br>However, in this form the target is non-stationary. To solve this problem, we instead use $I=(r+\gamma \underset{a’}{\max}Q(s’,a’,\textbf{w}^-)-Q(s,a,\textbf{w}))^2$ where $\textbf{w}^-$ is held fixed. Then simply apply SGD to train the neural network.</p>
<h3 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h3><p>To remove upward bias, the objective function is rewritten as<br>$$I=(r+\gamma Q(s’,\underset{a’}{\operatorname{argmax}} Q(s’,a’,\textbf{w}),\textbf{w}^-)-Q(s,a,\textbf{w}))^2$$<br>so that we can use the current Q-network to select actions, and use older Q-network to evaluate actions. The main difference lies in the argmax part.</p>
<h3 id="Prioritised-Replay"><a href="#Prioritised-Replay" class="headerlink" title="Prioritised Replay"></a>Prioritised Replay</h3><p>Experience is stored in priority queue according to surprise<br>$$|r+\gamma \underset{a’}{\max}Q(s’,a’,\textbf{w}^-)-Q(s,a,\textbf{w})|$$<br>so that the training can be more efficient, since more attention is paid to the poorly evaluated experience.</p>
<h3 id="Duelling-Network"><a href="#Duelling-Network" class="headerlink" title="Duelling Network"></a>Duelling Network</h3><p>The Q-network can be split into two channels:<br>$$Q(s,a)=V(s,\textbf{v})+A(s,a,\textbf{w})$$</p>
<ul>
<li><p>Action-independent value function V(s,<strong>v</strong>)</p>
</li>
<li><p>Action-dependent advantage function A(s,a,<strong>w</strong>)<br>​</p>
</li>
</ul>
<h2 id="Policy-based-Deep-RL"><a href="#Policy-based-Deep-RL" class="headerlink" title="Policy-based Deep RL"></a>Policy-based Deep RL</h2><p>The policy can also be represented directly by a neural network with weights <strong>u</strong>,<br>$$a = \pi(a|s,\textbf{u}) \quad \mathrm{or} \quad a=\pi(s,\textbf{u})$$<br>The objective function can then be written as total discounted reward<br>$$L(\textbf{u})=\mathbb{E}[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\dots|\pi(·,\textbf{u})]$$<br>Then train the network end-to-end using SGD.</p>
<h3 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h3><ul>
<li>The gradient of a stochastic policy is given by</li>
</ul>
<p>$$\frac{\partial L(\textbf{u})}{\partial \textbf{u}}=\mathbb{E}[\frac{\partial \log \pi(a|s,\textbf{u})}{\partial \textbf{u}}Q^{\pi}(s,a)]$$</p>
<p>Proof:</p>
<blockquote>
<p>$\quad L(\textbf{u})=\int \pi(a|s,\textbf{u}) Q^{\pi}(s,a)\mathrm{d}a$</p>
<p>$\quad \frac{\partial L(\textbf{u})}{\partial \textbf{u}}=\frac{\partial}{\partial \textbf{u}}\int \pi(a|s,\textbf{u}) Q^{\pi}(s,a)\mathrm{d}a=\int \frac{\partial}{\partial \textbf{u}}\pi(a|s,\textbf{u}) Q^{\pi}(s,a)\mathrm{d}a$</p>
<p>$\quad \mathrm{since} \qquad \frac{\partial}{\partial \textbf{u}}\pi(a|s,\textbf{u}) =\pi(a|s,\textbf{u})\frac{\partial \log \pi(a|s,\textbf{u})}{\partial \textbf{u}}$</p>
<p>$\quad \frac{\partial L(\textbf{u})}{\partial \textbf{u}}=\int \pi(a|s,\textbf{u})\frac{\partial \log \pi(a|s,\textbf{u})}{\partial \textbf{u}} Q^{\pi}(s,a)\mathrm{d}a=\mathbb{E}[\frac{\partial \log \pi(a|s,\textbf{u})}{\partial \textbf{u}}Q^{\pi}(s,a)]$</p>
</blockquote>
<ul>
<li>The gradient of a deterministic policy is given by </li>
</ul>
<p>$$\frac{\partial L(\textbf{u})}{\partial \textbf{u}}=\mathbb{E}[\frac{\partial Q^{\pi}(s,a)}{\partial a}\frac{\partial a}{\partial \textbf{u}}]$$</p>
<p>Proof can be given by the chain rule of derivatives. </p>
<h3 id="Actor-Critic-Algorithm"><a href="#Actor-Critic-Algorithm" class="headerlink" title="Actor-Critic Algorithm"></a>Actor-Critic Algorithm</h3><p>There are two neural networks here:</p>
<ul>
<li>The <strong>critic</strong> estimates value function $Q(s,a,\textbf{w})\approx Q^{\pi}(s,a)$</li>
<li>The <strong>actor</strong> updates the best policy by stochastic gradient ascent</li>
<li>Stochastic: $\frac{\partial l}{\partial \textbf{u}}=\frac{\partial \log \pi(a|s,\textbf{u})}{\partial \textbf{u}}Q(s,a,\textbf{w})$</li>
<li>Deterministic: $\frac{\partial l}{\partial \textbf{u}}=\frac{\partial  Q(s,a,\textbf{w})}{\partial a}\frac{\partial a}{\partial \textbf{u}}$</li>
</ul>
<h3 id="Asynchronous-Advantage-Actor-Critic-A3C"><a href="#Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="Asynchronous Advantage Actor-Critic (A3C)"></a>Asynchronous Advantage Actor-Critic (A3C)</h3><ul>
<li><strong>Asynchronous</strong>: Several actors exploring the same environment, and  value function is given by averaging over rewards.</li>
</ul>
<p>$$<br>V(s,\textbf{v}) \approx \mathbb{E}[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\dots|s]<br>$$</p>
<p>Therefore, several settings can be explored and the samples are less biased.</p>
<ul>
<li><p><strong>Advantage</strong>: The objective function uses the difference between Q-value and value function instead of Q-value itself, which estimates the advantage of the particular step the actor is going to take. </p>
<p>  The Q-value is estimated by an n-step sample<br>$$q_t= r{t+1}+\gamma r{t+2}+\dots+\gamma^{n-1} r{t+n}+\gamma^nV(s{t+n},\textbf{v})$$<br>  And the update of the <strong>actor</strong> is given by<br>$$\frac{\partial l_u}{\partial \textbf{u}}=\frac{\partial \log \pi(a_t|s_t,\textbf{u})}{\partial \textbf{u}}(q_t-V(s_t,\textbf{v}))$$<br> The advantage update makes the algorithm more precise when the training starts to converge and $q_t$ becomes generally very big.</p>
</li>
<li><p><strong>Actor-critic</strong>: The update of the actor has been shown above. The <strong>critic</strong> is updated to minimize the following MSE<br>$$l_v=(q_t-V(s_t,\textbf{v}))^2$$</p>
</li>
</ul>
<h3 id="Deep-Deterministic-Policy-Gradients-DDPG"><a href="#Deep-Deterministic-Policy-Gradients-DDPG" class="headerlink" title="Deep Deterministic Policy Gradients (DDPG)"></a>Deep Deterministic Policy Gradients (DDPG)</h3><p>This is the continuous analog of DQN, which uses experience replay (i.e. build dataset from agent’s experience).</p>
<p>In high-dimensional continuous action spaces, we cannot easily compute $\underset{a}\max Q(s,a)$, but luckily the actor-critic algorithms learn without max! </p>
<ul>
<li><strong>critic</strong>: $\quad I_w=(r+\gamma Q(s’,\pi(s’,\textbf{u}^-),\textbf{w}^-)-Q(s,a,\textbf{w}))^2$</li>
<li><strong>actor</strong>: $\quad\frac{\partial l_u}{\partial \textbf{u}}=\frac{\partial  Q(s,a,\textbf{w})}{\partial a}\frac{\partial a}{\partial \textbf{u}}$</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Last modified&lt;/em&gt;    31 Aug, 2017 @YuhZheng&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reference&lt;/em&gt;    “Tutorial: Deep Reinforcement Learning” by Dav
    
    </summary>
    
    
  </entry>
  
</feed>
